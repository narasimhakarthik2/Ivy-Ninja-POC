During my 2019 internship at JL Design Partners, an architecture firm in Sacramento, I was tasked with developing an Augmented Reality application for visualizing building designs. At that point, I was already intrigued by AR’s possibilities in diverse fields, from entertainment to aiding surgeons. Then, after two weeks of writing C# in Unity and establishing a workflow from Autodesk Revit, I loaded the prototype onto my phone and ecstatically invited my colleagues to try. Immediately, however, as I watched them rotate the virtual 3D structure by swiping on a 2D screen, I understood (with dismay) how counterintuitive the interaction was. At the time, I realized that products like daily-wear AR glasses could make architectural visualization more intuitive, but due to challenges with optics, batteries, and spatial understanding, these do not yet exist. Now, I wonder if solving these problems will be enough for widespread adoption at all. To what extent are we clinging to formats that do not present an effective way to interact with virtual content?

Which type of interface most complements AR? Which most improves cognitive processing and decision-making? Brain-Computer Interfaces are the common answer, but if so, what is the best approach to combining AR and BCI? What kind of neural activity should be employed? How can we design AR experiences for reactive BCIs? These questions lingered in my mind in the months that followed my internship. They remained there well after earning my degree in Computer Science, and even today, they resonate with the very same frequency. This is precisely why I seek admission to the Master’s Program in Computer Science at Stark University: to ascertain the optimal approach for virtual interactions that reduce cognitive load, enrich the senses, and augment human capabilities.

Perhaps no other graduate program in the world is more uniquely suited to helping me explore this niche within Human-Computer Interaction. The variety of interdisciplinary research carried out at the Stark HCI Lab, and the fact that it is one of the few funded master’s program that allow specialization in HCI, all compel my decision to apply. Certainly, coursework such as MCS731 Constructing Cross Reality Applications and MCS728 Affective Computing will prove essential as I begin to comprehend the core principles and design guidelines required for a thesis studying responsive and instinctual interactivity. Most importantly, however, is the opportunity to conduct research under Dr. Bruce Banner, Dr. Erik Selvig, and Dr. Jane Foster, whose work aligns perfectly with my goals of enhancing human cognition.

Particularly intriguing is Dr. Banner’s study on Electrodermal Activity and its relationship to physiological arousal. I hope to explore how this can be implemented as a metric to uncover cognitive loads of atypical input modalities. I am also captivated by Dr. Selvig’s work in augmenting user perception of the environment, and hope to investigate how eye-tracking can strengthen the system’s perception of user intent in HearThere, and how the sensor network in Tesseract can be used for augmented on-site exploration through persistent anchors. Additionally, I am open to working under Dr. Foster as her projects on musical expression and therapy (Vocal Waves) are similar to my ongoing work, Fluid Fingers. Having worked on numerous similar projects, I am confident I can make substantial contributions to these and other projects at Stark, while simultaneously exploring esoteric approaches like functional near-infrared spectroscopy for its superior spatial resolution (as compared to EEG) and resistance towards motion artifacts.

Perfect as the opportunity may seem, I am well aware that the daunting problems of AR require far more than general competence. Fortunately, my academic and research experiences have equipped me with the tools to face them. I was introduced to immersive technology in undergraduate coursework at Empire State University, such as Computer Graphics and Virtual and Augmented Reality. The Interaction Design Specialization from UC San Diego (via Coursera) further improved my understanding of Ideation, Prototyping, and Evaluation methods. Additionally, during my final undergraduate year, I developed a framework, ILMR, that allowed for prototyping hand interactions through direct manipulation with virtual content. Inspired by Google Cardboard, I conceived it using inexpensive and accessible modules like the Zeiss OnePlus headset and the Leap Motion Controller. However, it lacked depth perception and haptic feedback, which are vital in letting the user know they have successfully interacted with virtual content. Through a collaboration with Dr. Otto Octavius, a postdoctoral researcher at the Osborn Private Science Institute, we developed an interface, OptiTouch, that provided the semblance of haptic feedback using only visual cues.

My most rewarding experience, however, was not with ILMR, but rather the research I conducted unofficially with Professor Charles Xavier after graduation. I was intrigued by Neurofeedback and its non-pharmacological approach towards mental health therapy, but felt the feedback modality lacked appeal. Keeping ADHD-diagnosed children in mind, we devised an AR telekinetic experience where a user could bend a virtual spoon by reaching the desired psychological state. This work was accepted to the International Symposium on Mixed and Augmented Reality, where AR academia veteran Dr. Reed Richards claimed it to be impressive.

In my free time, I actively participate in hackathons, where I have led teams to multiple triumphs and developed multiple AR apps. One such was a music visualizer, VizAR, now published on the App Store. I redesigned the particle system in VizAR for Head-Mounted Displays, and enabled hand-particle interactions by implementing a Signed Distance Field. This ultimately led to my work, Fluid Fingers, which aims to elicit emotional states akin to those exhibited in live music performances. Considering these experiences, I feel confident that I am prepared to hone my abilities even further at the Stark Lab.

During my architectural design internship two years ago, I saw immediately the great chasm between our current technological capabilities with AR, and normal users’ ability to employ them. The questions that arose have been my guiding force ever since. This is precisely why I seek to contribute to the field of Human-Computer Interaction, and eventually work as a researcher in industry, ensuring AR’s impact in people’s everyday lives. To begin this effort in the Stark MS CS program would be the most tremendous honor of my career.
